{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7855358-4376-462c-b104-458e126e3dac",
   "metadata": {},
   "source": [
    " # Project Name - Apple and Google tweet Sentiment Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f16a357-49e5-4059-af66-fdaf839ff46c",
   "metadata": {},
   "source": [
    "## 1. Business Understanding\n",
    "### 1.1 Business Overview\n",
    "In the modern technological era, social media platforms such as Twitter(x) have become powerful sources where users share real-time opinions on brands and products. Companies such as Apple and Google, both global leaders  in technology and innovation, benefit greatly from understanding these public sentiments. Analyzing tweets about them helps reveal consumer opinions, trends and brand perceptions. Since manually reviewing  thousands of tweets is inefficient, automated sentiment analysis provides an effective solution. Classifing tweets as positive,negative or neutral to help companies monitor reputation, improve customer satisfaction  and make informed strategic decisions.\n",
    "\n",
    "### 1.2 Problem Statement\n",
    "Twitter(x) is a space where people share their opinions about brands and products. For global technology companies like Apple and Google, these tweets offer valuable insights into customer satisfaction, brand reputation and customer loyalty. However, the large volume of unstructured data makes it difficult to manually analyze the public sentiment in real time. To solve this problem, this project aims at developing an automated sentiment analysis model using Natural Language Processing(NLP) to classify tweets as positive, negative or neutral. This will help the companies better understand consumer perception, respond to feedback quickly and generally improve their products and overall Brand Image.\n",
    "### 1.3 Business Objectives \n",
    " #### 1.3.1 Main Objective\n",
    "To build a model that can rate the sentiment of a Tweet based on its content\n",
    " #### 1.3.2 Specific Objectives\n",
    "* To establish patterns and relationships between tweet content and corresponding sentiment categories.\n",
    "* To identify whether the special characters potray meaningful info.\n",
    "* To determine the main sentiment drivers.\n",
    "* To determine which words, phrases or subjects have the greatest influence on whether people see a brand favourably or unfavourably.\n",
    "* To generate meaningful insights that reflect customer attitude and brand perception in real time.\n",
    "\n",
    " #### 1.3.3 Research Questions\n",
    "1. What patterns and relationships exist between tweet content and the sentiment categories?\n",
    "2. Do special characters such as @, # and links carry any meaningful information that affects tweet sentiment?\n",
    "3. What specific features are the main targets of users' emotions towards apple and google?\n",
    "4. Which machine learning model performs the best in classifying tweet sentiment based on metrics such as accuracy, F1-score, precision and recall?\n",
    "5. What are the main words, phrases or themes that drive positive/negative sentiment towards these brands and how do these patterns change over time?\n",
    "\n",
    "### 1.4 Success Criteria\n",
    "* The project will be successful if it develops an accurate and reliable sentiment classification model that achieves an F1-weighted of 75%% and above and maintains balanced precision and recall across all the sentiment classes.\n",
    "* Success will also be measured by the model's ability to generalize well to unseen data, minimize missclassification between positive and negative tweets and provide actionable insights that help improve customer services and management of the brand.mrab\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeaed45-37af-43a0-8484-46e7f45cf256",
   "metadata": {},
   "source": [
    "## 2. Data Understanding\n",
    "### 2.1 Data source and Description\n",
    "- **Source:** This dataset is from CrowdFlower via <a href=\"https://data.world/crowdflower/brands-and-product-emotions\" target=\"_blank\">data.world</a> containing human raters sentiments.\n",
    "\n",
    "- **Description** The dataset has sentiments from over 9000 twitter users with each row containing a  users tweet_text, emotion_in_tweet_is_directed_at and emotion. Our main target is to use the text and train our model to predict the emotion from the tex\n",
    "\n",
    "### 2.2 Shape\n",
    "* The dataset shape is (9093, 4).\n",
    "* The dataset contains the following columns:\n",
    "1. tweet_text\n",
    "2. emotion_in_tweet_is_directed_at\n",
    "3. is_there_an_emotion_directed_at_a_brand_or_product\n",
    "   \n",
    "### 2.3 Datatypes\n",
    "  * All the columns have object dtype.  t.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078439f1-1f00-4114-9082-73285e6c1034",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6b0511-c88d-4403-af54-a06bc202f1ae",
   "metadata": {},
   "source": [
    "### 3.1 Data Loading\n",
    "* Import necessary libraries\n",
    "* Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf08250-93e5-4f7a-aef0-f5113d1dd8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary libraries\n",
    "# importing the necessary libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import label_binarize, StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_curve, auc, f1_score,precision_score,recall_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde03a82-d708-46a9-a7f8-7bc38213bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Dataset \n",
    "df = pd.read_csv('judge-1377884607_tweet_product_company.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08633e36-cfe8-4b49-8c48-7f3fb4202c8a",
   "metadata": {},
   "source": [
    "### 3.2 Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ece788-8262-4310-a94f-33b5e03a3670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# previewing the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90629e5-4498-442a-aba1-64105abbb2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the dimension of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dde0bdb-672e-44ba-a857-c70a7dcb9682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the overview of the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca806a0-ada7-433a-bb6b-a5e4436b3390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5beb84-7c32-4334-a9b0-683f4e6142ae",
   "metadata": {},
   "source": [
    "* We have missing values in tweet_text and emotion_in_tweet_is_directed_at columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deef24a-f5c6-4b21-bd43-990aff292257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013205e9-0100-47b0-8ae7-2f4beab0abd1",
   "metadata": {},
   "source": [
    "* We have 22 duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd4b8ee-a676-4bc3-ae80-6b1d179d2a78",
   "metadata": {},
   "source": [
    "### 3.3 Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7007fb-5054-4bb6-82d1-21639788e36b",
   "metadata": {},
   "source": [
    "#### 3.3.1 Handling duplicates\n",
    "* Since duplicate tweets can bias the model by overrepresenting a particular sentiment, we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a03cc53-5bea-4c07-949e-010717836fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing duplicates\n",
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28daf93-71df-4b80-aad5-9191dac45b3f",
   "metadata": {},
   "source": [
    "#### 3.3.2 Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf74161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missingness analysis\n",
    "missing_brand = df['emotion_in_tweet_is_directed_at'].isna().sum()\n",
    "total_length = len(df)\n",
    "print(f\"Missing data on where emotion is directed to: {missing_brand}/{total_length} ({missing_brand/total_length:.1%})\")\n",
    "\n",
    "# Plot for the non-missing values\n",
    "directed_at_counts = df['emotion_in_tweet_is_directed_at'].value_counts().head(10)\n",
    "\n",
    "if not directed_at_counts.empty:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    directed_at_counts.plot(kind='barh')\n",
    "    plt.title('Top 10 Directed At Mentioned')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Brand/Product')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No non-missing values to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3df691",
   "metadata": {},
   "source": [
    "Here, users seem to have a emotions towards iPas Apple and google products consequtively as compared to other brands and products. The column seem to have a huge, missing value percentage of 63.8%. Since we would like to see how the brand or product affects the users we have to impute the missing values in 'emotion_in_tweet_is_directed_at' with 'Unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a774a-3e03-4fa0-9796-02070070aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing for emotion_in_tweet_is_directed_at with unknown since we would like to see how the brand or product affects users\n",
    "df['emotion_in_tweet_is_directed_at'].fillna('unknown',inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66310a28",
   "metadata": {},
   "source": [
    "#### NB \n",
    "We will be ignoring the 'Unknown' during our interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc05900-0c9e-4059-a62e-d25ceea734c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the missing value in tweet_text\n",
    "df = df.dropna(subset=['tweet_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e6764e-e0ec-4a99-bd29-6046ae95a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Combine the two sentiment categories into one called 'Neutral'\n",
    "df['is_there_an_emotion_directed_at_a_brand_or_product'] = df['is_there_an_emotion_directed_at_a_brand_or_product'].replace({\n",
    "    \"No emotion toward brand or product\": \"Neutral\", \n",
    "    \"I can't tell\": \"Neutral\"\n",
    "})\n",
    "\n",
    "# Check the updated value counts\n",
    "print(df['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fba86d3",
   "metadata": {},
   "source": [
    "In the above we basically combining the two columns because they mean the same thing, the 'No emotion toward brand or product' and 'I can't tell'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47888d2f-3517-461f-96f7-d804a8d09484",
   "metadata": {},
   "source": [
    "### 3.4 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b1c661-5046-46b1-a31f-0c22f56fd698",
   "metadata": {},
   "source": [
    "#### 3.4.1 Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4648861-be75-4ebd-a470-5c1cc789ad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of emotions in the dataset\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "df['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts().plot(kind='bar')\n",
    "plt.title('Emotion Distribution')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title('Emotion Proportions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70353952",
   "metadata": {},
   "source": [
    "The above shows the distribution of our target variable ~'s_there_an_emotion_directed_at_a_brand_or_product'. It seems to be imbalanced with the neutral being the majority and negative having the minority representation of datapoints. positive emotion is moderately represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432296d5-a098-4f67-9b5e-e79424817995",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emotion_in_tweet_is_directed_at'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31923961",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91adbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulation with sentiment\n",
    "print(\"CROSS-TABULATION: Emotion Target vs Sentiment\")\n",
    "print(\"_\" * 60)\n",
    "crosstab = pd.crosstab(\n",
    "    data['emotion_in_tweet_is_directed_at'], \n",
    "    data['is_there_an_emotion_directed_at_a_brand_or_product'],\n",
    "    margins=True\n",
    ")\n",
    "print(crosstab)\n",
    "\n",
    "# Heatmap of Cross-tabulation\n",
    "fig, axes = plt.subplots(figsize=(5,4))\n",
    "crosstab_normalized = pd.crosstab(\n",
    "    data['emotion_in_tweet_is_directed_at'], \n",
    "    data['is_there_an_emotion_directed_at_a_brand_or_product'],\n",
    "    normalize='index'\n",
    ")\n",
    "sns.heatmap(crosstab_normalized, annot=True, fmt='.2%', cmap='YlOrRd', axes=axes)\n",
    "axes.set_title('Emotion Target vs Sentiment -Normalized', fontsize=14, fontweight='bold')\n",
    "axes.set_xlabel('Sentiment')\n",
    "axes.set_ylabel('Emotion Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7c0ded",
   "metadata": {},
   "source": [
    "Generally the positive emotion seem to have the highest influence on all brands and products it leads in position followed by negative emotion, no emotiontowards a brand takes the 3rd position and i can't tell has the least percentages towards all the brands and products and nothing at all on others. Android app and android products are the most infuenced by positive emotion, this might mean that users are more likely to use these products or brand. Iphone has the highest negative emotion though only a 34%, it means some users are not very satisfied with the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64f1d1d-1ef8-4a78-8d63-e43a54e97d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating word count\n",
    "df = df.copy() \n",
    "df['char_count'] = df['tweet_text'].str.len()\n",
    "df['word_count'] = df['tweet_text'].apply(lambda x: len(str(x).split()))\n",
    "print(df[['tweet_text', 'char_count', 'word_count']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b377d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does special characters @.# and link potry meaningful information?\n",
    "from collections import Counter\n",
    "# 1. User Mentions (@) Analysis\n",
    "def extract_mentions(text):\n",
    "    #Extract all @mentions from text\n",
    "    if isinstance(text, str):\n",
    "        return re.findall(r'@\\w+', text.lower())\n",
    "    return []\n",
    "\n",
    "# Extracting all  the mentions\n",
    "all_mentions = []\n",
    "for text in data['tweet_text']:\n",
    "    all_mentions.extend(extract_mentions(text))\n",
    "\n",
    "# Count most common mentions\n",
    "mention_counter = Counter(all_mentions)\n",
    "top_mentions = mention_counter.most_common(10)\n",
    "\n",
    "print(\"Top 10 most mentioned user/brands\")\n",
    "print(\"=\" * 60)\n",
    "for mention, count in top_mentions:\n",
    "    print(f\"{mention}: {count} times\")\n",
    "\n",
    "# Mentions by sentiment\n",
    "print(\"Top mention by sentiment\")\n",
    "print(\"=\" * 60)\n",
    "sentiments = data['is_there_an_emotion_directed_at_a_brand_or_product'].unique()\n",
    "for sentiment in sentiments:\n",
    "    sentiment_data = data[data['is_there_an_emotion_directed_at_a_brand_or_product'] == sentiment]\n",
    "    sentiment_mentions = []\n",
    "    for text in sentiment_data['tweet_text']:\n",
    "        sentiment_mentions.extend(extract_mentions(text))\n",
    "    \n",
    "    if sentiment_mentions:\n",
    "        top_sentiment_mentions = Counter(sentiment_mentions).most_common(10)\n",
    "        print(f\"\\n--- Sentiment: '{sentiment}' ---\")\n",
    "        for mention, count in top_sentiment_mentions:\n",
    "            print(f\"  {mention}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2633c962",
   "metadata": {},
   "source": [
    "Special character @ seem to have meaningful information as we can see the most tagged sentiment in relation to brand or product in as much there is no direct link in most cases for emotion to the tagged user or brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e2cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Hashtags (#) Analysis\n",
    "def extract_hashtags(text):\n",
    "    #Extract all #hashtags from text\n",
    "    if isinstance(text, str):\n",
    "        return re.findall(r'#\\w+', text.lower())\n",
    "    return []\n",
    "\n",
    "# Extracting  all hashtags\n",
    "all_hashtags = []\n",
    "for text in data['tweet_text']:\n",
    "    all_hashtags.extend(extract_hashtags(text))\n",
    "\n",
    "# Count most common hashtags\n",
    "hashtag_counter = Counter(all_hashtags)\n",
    "top_hashtags = hashtag_counter.most_common(10)\n",
    "\n",
    "print(\"Top 10 most popular hashtags\")\n",
    "print(\"=\" * 60)\n",
    "for hashtag, count in top_hashtags:\n",
    "    print(f\"{hashtag}: {count} times\")\n",
    "\n",
    "# Hashtags by sentiment\n",
    "print(\"Top Hashtags by Sentiments\")\n",
    "print(\"=\" * 60)\n",
    "for sentiment in sentiments:\n",
    "    sentiment_data = data[data['is_there_an_emotion_directed_at_a_brand_or_product'] == sentiment]\n",
    "    sentiment_hashtags = []\n",
    "    for text in sentiment_data['tweet_text']:\n",
    "        sentiment_hashtags.extend(extract_hashtags(text))\n",
    "    \n",
    "    if sentiment_hashtags:\n",
    "        top_sentiment_hashtags = Counter(sentiment_hashtags).most_common(10)\n",
    "        print(f\"\\n Sentiment: '{sentiment}' \")\n",
    "        for hashtag, count in top_sentiment_hashtags:\n",
    "            print(f\"  {hashtag}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf741c1a",
   "metadata": {},
   "source": [
    "Special character # seem to have a meaningful information in regard to product or brand in relation to emotion. Like in the above the top ten hashtags are in the negative emotion, this links us to the hashtags on the product and brand and sxsw seem to have the most being tagged and having a negative emotion. The preceeding products can be seen in the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a55f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.  Links (http/https) Analysis\n",
    "def contains_link(text):\n",
    "    #Check if text contains a URL\n",
    "    if isinstance(text, str):\n",
    "        return bool(re.search(r'http[s]?://\\S+', text))\n",
    "    return False\n",
    "\n",
    "# Count tweets with links\n",
    "data['contains_link'] = data['tweet_text'].apply(contains_link)\n",
    "total_with_links = data['contains_link'].sum()\n",
    "total_tweets = len(data)\n",
    "percentage_with_links = (total_with_links / total_tweets) * 100\n",
    "\n",
    "print(\"Link Analysis\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total tweets: {total_tweets}\")\n",
    "print(f\"Tweets with links: {total_with_links}\")\n",
    "print(f\"Percentage with links: {percentage_with_links:.2f}%\")\n",
    "\n",
    "# Links by sentiment\n",
    "print(\"\\n Links by Sentiment \")\n",
    "for sentiment in sentiments:\n",
    "    sentiment_data = data[data['is_there_an_emotion_directed_at_a_brand_or_product'] == sentiment]\n",
    "    links_count = sentiment_data['contains_link'].sum()\n",
    "    sentiment_total = len(sentiment_data)\n",
    "    percentage = (links_count / sentiment_total * 100) if sentiment_total > 0 else 0\n",
    "    print(f\"{sentiment}: {links_count}/{sentiment_total} ({percentage:.2f}%)\")\n",
    "\n",
    "fig ,axes = plt.subplots(figsize=(5,4))\n",
    "link_by_sentiment = data.groupby('is_there_an_emotion_directed_at_a_brand_or_product')['contains_link'].sum()\n",
    "link_by_sentiment.plot(kind='bar', ax=axes, color='lightgreen')\n",
    "axes.set_title('Number of Tweets with Links by Sentiment', fontsize=14, fontweight='bold')\n",
    "axes.set_xlabel('Sentiment')\n",
    "axes.set_ylabel('Count of Tweets with Links')\n",
    "axes.tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681a49af",
   "metadata": {},
   "source": [
    "Ageneral observation can be made that there is no much impact on links on tweets. But the few tweets that has link with them show a relationship on tweet by sentiment either positive or no emotion towards a brand or product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e590c4d2-6e86-4386-8a13-b4c78c9f4646",
   "metadata": {},
   "source": [
    "### 3.5 Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3b0d20-1326-4f05-b5f1-c0a394f446f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading necessary resources\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    # remove mentions\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
    "    # remove hashtags\n",
    "    text = re.sub(r'#', '', text)\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a1609e-b640-4667-a7af-b9a8478da4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying text preprocessing function to the original tweet text\n",
    "df['cleaned_text'] = df['tweet_text'].apply(preprocess_text)\n",
    "print(df[['tweet_text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d63d5-6cc5-45ff-ae38-e1aeb670ca5c",
   "metadata": {},
   "source": [
    "### 3.6 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf33fe0-49d4-49cd-94c7-69586e028ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenizing all cleaned tweets into one list\n",
    "words = ' '.join(df['cleaned_text']).split()\n",
    "\n",
    "# Generating bigrams\n",
    "bigram_list = list(bigrams(words))\n",
    "\n",
    "# Counting most common bigrams\n",
    "bigram_counts = Counter(bigram_list)\n",
    "print(bigram_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5587b502-179c-4d45-9cf5-1229411524fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing bigrams\n",
    "bigram_df = pd.DataFrame(bigram_counts.most_common(10), columns=['bigram', 'count'])\n",
    "bigram_df.plot.bar(x='bigram', y='count', figsize=(10,5), title='Top 10 Bigrams')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d5701f-d811-4377-9e9c-9df8a3b63cb8",
   "metadata": {},
   "source": [
    "* Checking word association strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2315a929-14b3-4c7d-be6b-9d6c964527f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import BigramAssocMeasures\n",
    "\n",
    "# Initialize bigram measures and finder\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "\n",
    "# Filter bigrams that occur less than 50 times\n",
    "bigram_finder.apply_freq_filter(50)\n",
    "\n",
    "# Score bigrams using PMI\n",
    "bigram_pmi_scored = bigram_finder.score_ngrams(bigram_measures.pmi)\n",
    "\n",
    "# Display top bigrams\n",
    "print(bigram_pmi_scored[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729eb341-fcff-4eb3-acc1-0cceaf182c56",
   "metadata": {},
   "source": [
    "* Since the PMI of all the bigrams is high, it means that they are rare individually but frequent together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a0389e-0dd2-4455-94e0-af283b60241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing frequency distribution\n",
    "freq_dist = FreqDist(words)\n",
    "\n",
    "# Showing top 20 most common words\n",
    "print(freq_dist.most_common(20))\n",
    "\n",
    "# Visualizing\n",
    "plt.figure(figsize=(10,5))\n",
    "freq_dist.plot(20, title='Top 20 Most Common Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc555d6-fb9b-4be0-a095-877296684dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(background_color='white', random_state=21, max_font_size=40,scale=5).generate(str(df['cleaned_text']))\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e815d33-a63b-49c3-af98-f1204b78cfb3",
   "metadata": {},
   "source": [
    "* The two visualization show that **SXSW**, **link**, **rt**, **google**, **ipad**, **apple** and **Iphone** are the most common words in our preprocessed tweet dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7036f3a1-611d-4e4f-8537-b6268f3155e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Creating a new feature 'contains_sxsw' to check if the word 'sxsw' appears in each tweet\n",
    "df['contains_sxsw'] = df['cleaned_text'].apply(lambda x: 1 if 'sxsw' in x else 0)\n",
    "print(df['contains_sxsw'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786a33ca-8dc9-44cc-9630-508483650531",
   "metadata": {},
   "source": [
    "* 1 represents contains while 0 means does not contain \n",
    "*  SXSW(South by southwest event) is dominating the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9200b1c8-5930-4529-b3c5-8399bc4f113a",
   "metadata": {},
   "source": [
    "* Almost all tweets contain sxsw.\n",
    "* Since it's more of a contextual keyword for the event, we are going to drop given that it does not add sentiment meaning.It can bias TF-IDF and other vectorization methods, making the model focus on 'sxsw' instead of meaningful product-related words.\n",
    "* We are also going to drop rt(which is a structure and it appears when someone retweets a tweet) and link(placeholder for URls) since our focus are on brands sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f1e601-4f86-491c-90ae-1650acfe1ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping sxsw, link, rt\n",
    "df['cleaned_text'] = df['cleaned_text'].str.replace(r'\\b(sxsw|rt|link)\\b', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eb095b-9002-4140-b702-7d3e90ea4a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting 2x2 grid word cloud for each sentiment\n",
    "plt.figure(figsize=(40,20))\n",
    "\n",
    "for index, col in enumerate(df['is_there_an_emotion_directed_at_a_brand_or_product'].unique()):\n",
    "    plt.subplot(2,2, index+1)\n",
    "    # printing col\n",
    "    df1 = df[df['is_there_an_emotion_directed_at_a_brand_or_product']==col]\n",
    "    data = df1['cleaned_text']\n",
    "    wordcloud = WordCloud(background_color='white', max_words=500, max_font_size=40, scale=5).generate(str(data))\n",
    "\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.title(col, fontsize=40)\n",
    "    \n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bf692b-9fa6-4b68-9899-703ff6f85208",
   "metadata": {},
   "source": [
    "**Google**, **Ipad**, **apple**, **iphone** are the most common words now. Brand is the main topic of debate, with tweets also concentrating on the type of brand and the functionality of the program. Frequent use of phrases like \"appreciate\" may indicate a generally favorable emotion, whereas the use of words like \"dead\" may imply comments about a specific brand or app performing poorly, which would indicate a negative emotion. Following extensive text cleaning, the visualization offers a quick, understandable overview of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47b8316-efe1-4361-a7f7-21db6629228a",
   "metadata": {},
   "source": [
    "### **Word Cloud Analysis by Emotion Category**\n",
    "\n",
    "To illustrate the distinctive words that characterize each emotional category, word clouds representing the three sentiment categories are compared.\n",
    "\n",
    "**Positive emotion**\n",
    "The word cloud for positive emotions shows words related to contentment. Words like **beautifully**, **simple**  have been used to convey approval, pleasure, and good experiences.\n",
    "\n",
    "**Negative emotion** \n",
    "Words that convey discontent and criticism can be found in the negative emotion word cloud. Words like **crashing**, **dead**, draw attention to grievances, issues, and bad experiences. This terminology frequently alludes to problems, setbacks, and undesirable results.\n",
    "\n",
    "**Neutral emotion**\n",
    "Factual and objective terminology can be found in the neutral emotion word cloud.  Common terms like \"google,\" \"app,\" \"third,\" \"store,\" and \"Android\" stand for broad knowledge, context, and observational content that isn't highly emotionally charged.  These phrases usually don't have a definite positive or negative bias and represent commonplace circumstances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172b3e3b-a69c-4fad-a75f-d5312dba7ecf",
   "metadata": {},
   "source": [
    "### **Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266ec05a-63ff-4487-8090-478d021e5f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=20)\n",
    "\n",
    "# Fit and transform\n",
    "tfidf_vectors = tfidf.fit_transform(df['cleaned_text'])\n",
    "\n",
    "# Create DataFrame of TF-IDF scores\n",
    "tfidf_df = pd.DataFrame(tfidf_vectors.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "# Display top TF-IDF words\n",
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1a8e4c-6e72-45d3-8af6-0e6f545e7f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing vectorizer with high dimensions\n",
    "tfidf = TfidfVectorizer(max_features=3000)\n",
    "X_tfidf = tfidf.fit_transform(df['cleaned_text'])\n",
    "X_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecffd958-365a-4a60-bd15-ad1fd6395ac5",
   "metadata": {},
   "source": [
    "* Visualizing high dimensional tweet-data reduced to two main components(PCA1 and PCA2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd2246c-afd1-4cd8-97c3-9c5ecf42516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_tfidf.toarray())\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "plt.title(\"PCA Visualization of Tweet Topics\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15366d3-2b92-4db4-8eaa-fa384a34b2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if we are working with a sparse matrix\n",
    "import scipy.sparse\n",
    "# Checking type\n",
    "type(X_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e5df6-ef21-4c84-8c41-0cf8c3ec7332",
   "metadata": {},
   "source": [
    "* For PCA and clustering, we will convert our compressed sparse matrix to dense using .toarray()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e35328-9cd7-40ca-868d-e7ee2391dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Applying PCA to reduce dimensionality for visualization and speed\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_tfidf.toarray())\n",
    "\n",
    "# KMeans clustering with 5 clusters\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans.fit(X_pca)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Add cluster labels to your dataframe\n",
    "df['cluster'] = labels\n",
    "\n",
    "# Visualizing PCA components cluster colors\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='tab10', s=30)\n",
    "plt.title('KMeans Clusters (after PCA)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5bb97c-ab4c-464d-9f73-c20f9ef38b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602e65af-b10f-45d8-aa5b-7e3b20240df8",
   "metadata": {},
   "source": [
    "# 4. Modelling\n",
    "In this section, we will build and evaluate different machine learning models to predict the sentiment of tweets related to Apple and Google.\n",
    "The primary goal is to determine which algorithm best captures the emotional tone of user tweets whether positive, negative, or neutral based on their textual content.\n",
    "\n",
    "We use both classical machine learning algorithms (Logistic Regression,Random Forest, Naive Bayes, and XGBoost) and deep learning approaches (CNN-LSTM) to capture linguistic and contextual features in the data.\n",
    "Each model is trained using TF-IDF vectorized text data, and in (CNN-LSTM), embedding-based representations are used to improve context capture.\n",
    "\n",
    "#### Methodology\n",
    "\n",
    "Splitting the dataset into training, validation, and test sets.\n",
    "\n",
    "Train each model and perform hyperparameter tuning using GridSearchCV where applicable.\n",
    "\n",
    "Evaluate models on validation data to select the best one.\n",
    "\n",
    "Conduct final performance evaluation on the test set later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c13b0b-e66b-49ae-804c-4f053b8685d1",
   "metadata": {},
   "source": [
    "# 4.1 Logistic Classification\n",
    "Logistic Regression is a strong baseline for text classification tasks.\n",
    "It works well with TF-IDF features and can efficiently separate positive and negative sentiments based on word frequencies.\n",
    "We use this model as a benchmark to evaluate whether more complex algorithms provide significant improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee77020d-a443-46fc-8337-e1bf126ca8aa",
   "metadata": {},
   "source": [
    "### 4.1.1 Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a495b5d8-ba44-4bcc-927a-4e280fc478a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming is_there_an_emotion_directed_at_a_brand_or_product to sentiment\n",
    "df = df.rename(columns={'is_there_an_emotion_directed_at_a_brand_or_product': 'sentiment'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e16dbd1-00da-417d-852b-b95420fbcf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064857c9-eef2-450d-a073-80a47b9835b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Filtering to binary classes\n",
    "df_binary = df[df['sentiment'].isin(['Positive emotion', 'Negative emotion'])].copy()\n",
    "df_binary.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Preparing data\n",
    "X_bi = df_binary['cleaned_text'].fillna('').astype(str)\n",
    "y_bi = df_binary['sentiment']\n",
    "\n",
    "# stratify=y ensures the class distribution in train/test sets is the same as in the full dataset.\n",
    "X_train, X_test, y_train, y_test = train_test_split( X_bi, y_bi , test_size=0.2, random_state=42)\n",
    "\n",
    "# Define pipeline\n",
    "bilog = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english')),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "bilog.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bilog.predict(X_test)\n",
    "\n",
    "# Evaluation Metrics \n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compute weighted metrics for comparison\n",
    "precision_val = precision_score(y_test, y_pred, average='weighted')\n",
    "recall_val = recall_score(y_test, y_pred, average='weighted')\n",
    "f1_val = f1_score(y_test, y_pred, average='weighted')\n",
    "accuracy_val = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nWeighted Precision: {precision_val:.3f}\")\n",
    "print(f\"Weighted Recall: {recall_val:.3f}\")\n",
    "print(f\"Weighted F1-score: {f1_val:.3f}\")\n",
    "print(f\"Accuracy: {accuracy_val:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f34bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Visualization \n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred),\n",
    "            annot=True, fmt='d', cmap='viridis',\n",
    "            xticklabels=bilog.classes_,\n",
    "            yticklabels=bilog.classes_)\n",
    "plt.title('Confusion Matrix â€” Logistic Regression (Binary Sentiment)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7855f940",
   "metadata": {},
   "source": [
    "Logistic Regression is used here as a baseline classifier for binary sentiment classification (Positive emotion vs Negative emotion).\n",
    "The model leverages a TF-IDF vectorizer to convert text into weighted numerical features, emphasizing distinctive words in each sentiment class.\n",
    "Logistic Regression is effective for binary problems, fast to train, and provides probabilistic interpretations of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85762bd3-0796-4451-ab50-18bbf99a948c",
   "metadata": {},
   "source": [
    "### 4.1.2 Multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd28b7-f6e9-4744-a18e-2ff6fc1d8d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "X = df['cleaned_text']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Define pipeline\n",
    "mltlog = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1,2))),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "mltlog.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aec7fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mltlog.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compute weighted metrics for comparison\n",
    "precision_val = precision_score(y_test, y_pred, average='weighted')\n",
    "recall_val = recall_score(y_test, y_pred, average='weighted')\n",
    "f1_val = f1_score(y_test, y_pred, average='weighted')\n",
    "accuracy_val = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nWeighted Precision: {precision_val:.3f}\")\n",
    "print(f\"Weighted Recall: {recall_val:.3f}\")\n",
    "print(f\"Weighted F1-score: {f1_val:.3f}\")\n",
    "print(f\"Accuracy: {accuracy_val:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa063e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f166f3b0-951b-4978-8e1e-eafd3b59c1e3",
   "metadata": {},
   "source": [
    "# 4.1.3 Applying **RandomOverSampler**  to ensure that the clases are balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4da7a11-05d1-4355-8634-31a4356a6492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# Data preparation\n",
    "X = df['cleaned_text']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Define pipeline with RandomOverSampler\n",
    "randomized = ImbPipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english')),\n",
    "    ('smote',SMOTE(random_state=42)),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "randomized.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aad2f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compute weighted metrics for comparison\n",
    "precision_val = precision_score(y_test, y_pred, average='weighted')\n",
    "recall_val = recall_score(y_test, y_pred, average='weighted')\n",
    "f1_val = f1_score(y_test, y_pred, average='weighted')\n",
    "accuracy_val = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nWeighted Precision: {precision_val:.3f}\")\n",
    "print(f\"Weighted Recall: {recall_val:.3f}\")\n",
    "print(f\"Weighted F1-score: {f1_val:.3f}\")\n",
    "print(f\"Accuracy: {accuracy_val:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6802625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='viridis',\n",
    "            xticklabels=randomized.classes_, yticklabels=randomized.classes_)\n",
    "plt.title('Confusion Matrix Randomized logistic Regression')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e9802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to train the model, make predictions and calculate evaluation metrics after hyperparameter tuning\n",
    "def modelling(pipe, is_grid_search=False):\n",
    "    # Fit the model (either regular pipeline or GridSearchCV)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best estimator if it's a GridSearchCV object\n",
    "    if is_grid_search:\n",
    "        print(\"Best Parameters:\", pipe.best_params_)\n",
    "        print(\"Best CV Score:\", pipe.best_score_)\n",
    "        print(\"\\n\")\n",
    "        model = pipe.best_estimator_\n",
    "    else:\n",
    "        model = pipe\n",
    "    \n",
    "    # Predict train and test data\n",
    "    y_hat_train = model.predict(X_train)\n",
    "    y_hat_test = model.predict(X_test)\n",
    "\n",
    "    # classification report\n",
    "    # For training set\n",
    "    print(\"Training Classification Report:\")\n",
    "    print(classification_report(y_train, y_hat_train))\n",
    "\n",
    "    # For testing set\n",
    "    print(\"Testing Classification Report:\")\n",
    "    print(classification_report(y_test, y_hat_test))\n",
    "\n",
    "    base_train_accuracy = accuracy_score(y_train, y_hat_train)\n",
    "    base_test_accuracy = accuracy_score(y_test, y_hat_test)\n",
    "\n",
    "    print(\"Difference between train and test accuracy\")\n",
    "    print(base_train_accuracy - base_test_accuracy)\n",
    "    \n",
    "    # Return the best estimator\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fe929a-acc0-414b-896c-f8c4eabe5f29",
   "metadata": {},
   "source": [
    "### **Hyper_parameter Tuning using GridSearchCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f08c876-f7af-4fcc-9e00-311c299ef861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [3000, 5000, 7000],\n",
    "    'clf__C': [0.1, 1]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(randomized, param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c073771-2874-4522-b8e3-35910c6e196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare data\n",
    "X = df['cleaned_text']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Split data with stratified class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Final pipeline with RandomOverSampler and tuned hyperparameters\n",
    "final_pipeline = ImbPipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=7000, stop_words='english')),\n",
    "    ('ros', RandomOverSampler(random_state=42)),  # Oversample only training data\n",
    "    ('clf', LogisticRegression(max_iter=1000, C=1, penalty='l2', solver='liblinear'))  \n",
    "])\n",
    "\n",
    "# Train and evaluate using your modelling function\n",
    "final_model = modelling(final_pipeline, is_grid_search=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faf0e41",
   "metadata": {},
   "source": [
    "# 4.2 Random Forest Classifier\n",
    "Random Forest is an ensemble of decision trees that improves predictive power through bootstrapping and random feature selection.\n",
    "It captures nonlinear relationships between words and sentiment that linear models might miss.\n",
    "While less interpretable, it often achieves strong performance without much tuning, making it a reliable baseline among tree-based methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520d975e-9c26-4b0a-b41f-1d73aeb49e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['cleaned_text']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93f7952-235a-4c20-9bad-3d69d4f058bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tdif',TfidfVectorizer(ngram_range=(1,2))),\n",
    "    #('smote',SMOTE(random_state=42)),\n",
    "    ('model',RandomForestClassifier(class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# modelling\n",
    "rdf = modelling(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e3eb5a",
   "metadata": {},
   "source": [
    "# 4.3 Multinomial Naive Bayes\n",
    "Naive Bayes is commonly used for text data because it assumes feature independence, which aligns well with bag-of-words and TF-IDF representations.\n",
    "Itâ€™s computationally efficient and performs particularly well when features (words) independently contribute to class probability.\n",
    "We include it to compare its simplicity and efficiency with Logistic Regression and more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f40c27-4ce9-4ab4-89ff-7d670444d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "pipeline.set_params(model = MultinomialNB())\n",
    "nb =  modelling(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa111de",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid= {'tdif__min_df': [1, 3, 5],\n",
    "'tdif__sublinear_tf': [True, False],\n",
    "'tdif__use_idf': [True, False],\n",
    "'tdif__ngram_range': [(1,1), (1,2)],\n",
    "'tdif__max_df': [0.75, 0.9],\n",
    "'model__alpha': [0.01, 0.1, 0.5],\n",
    "'model__fit_prior': [True, False]}\n",
    "\n",
    "\t\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#list of steps excluding 'smote'\n",
    "steps = [(name, step) for name, step in pipeline.steps if name != 'smote']\n",
    "\n",
    "#Rebuilding the pipeline after removing SMOTE\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "#set MultinomialNB\n",
    "pipeline.set_params(model=MultinomialNB())\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy',n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best accuracy Score:\", grid.best_score_)\n",
    "\n",
    "pipeline.set_params(model = MultinomialNB(alpha=0.1, fit_prior=True))\n",
    "pipeline.set_params(\n",
    "    tdif__ngram_range=(1, 2),\n",
    "    tdif__max_df=1.0,\n",
    "    tdif__min_df=1,\n",
    "    tdif__use_idf=False,\n",
    "    tdif__sublinear_tf=False,\n",
    "\n",
    ")\n",
    "multinomial_tuned= modelling(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596c181c",
   "metadata": {},
   "source": [
    "# 4.4 XGBoost Classifier\n",
    "XGBoost (Extreme Gradient Boosting) is a powerful ensemble algorithm that builds multiple decision trees iteratively to minimize classification errors.\n",
    "It can capture nonlinear relationships between words and sentiment better than linear models.\n",
    "We use XGBoost to assess whether boosting methods can improve predictive accuracy for context-heavy tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab88ba-ca1e-429a-aff9-6750dfedd7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGboost Model\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Dataset split into test and train sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    ngram_range=(1,2),\n",
    "    sublinear_tf=True  \n",
    ")\n",
    "\n",
    "X_train_vec = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_vec = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Handling class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vec, y_train)\n",
    "\n",
    "# Building the model and embeding gridsearch for hyperparametr tuning\n",
    "xgparam_grid = {\n",
    "    'n_estimators': [50,100],\n",
    "    'max_depth': [4, 6],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.5, 0.7],\n",
    "    'min_child_weight': [1,2]\n",
    "}\n",
    "\n",
    "xgb_base = XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric='mlogloss',\n",
    "    use_label_encoder=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65ce17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing grid search\n",
    "xgb_grid = GridSearchCV(\n",
    "    xgb_base,\n",
    "    xgparam_grid,\n",
    "    cv=3,  \n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "# Fitting themodel\n",
    "xgb_grid.fit(X_train_resampled, y_train_resampled)\n",
    "best_xgb = xgb_grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ccb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb = xgb_grid.best_estimator_\n",
    "\n",
    "# Predicting the model on the vectorized training set \n",
    "xgbtrain_pred = best_xgb.predict(X_train_resampled)\n",
    "# Testing the model on the vectorized test set\n",
    "xgbtest_pred = best_xgb.predict(X_test_vec)\n",
    "\n",
    "# Evaluating the model\n",
    "training_accuracy =accuracy_score(y_train_resampled,xgbtrain_pred)\n",
    "test_accuracy = accuracy_score(y_test,xgbtest_pred)\n",
    "\n",
    "print('')\n",
    "print('Training Accuracy: {:.2f}%'.format(training_accuracy * 100))\n",
    "print('Validation accuracy: {:.2f}%'.format(test_accuracy * 100)) \n",
    "print('Difference:',training_accuracy-test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dd6ee2",
   "metadata": {},
   "source": [
    "# 4.5 CNN-LSTM Model\n",
    "The CNN-LSTM model combines two powerful deep learning architectures:\n",
    "- Convolutional Neural Networks (CNN) detect local text features like phrases and n-grams.\n",
    "- Long Short-Term Memory networks (LSTM) capture sequential dependencies and context.\n",
    "This makes them suitable for complex language structures in tweets.\n",
    "We include this model to explore whether sequential modeling of text improves sentiment detection compared to traditional TF-IDF-based classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625805b8",
   "metadata": {},
   "source": [
    "Here we will be feeding the raw data into our model, to ensure consistency in feature extraction, and avoid data leakage.\n",
    "We first encode our target variable to ensure its in numeric form, CNN-LSTM only uses numerical data for feature extractions.\n",
    "We will split the data into training, validation and test sets. We then tokenize to convert text into numeric sequence then pad the sequence into a uniform 2D numeric matrix, ready for the embedding layer in the convolutional neural network.\n",
    "#### NB: Tokenizing is only done on the training data to avoid data leakage on the test and validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ce5b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df['cleaned_text']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Target Variable Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Original classes: {label_encoder.classes_}\")\n",
    "print(f\"Encoded labels: {y_encoded[:5]}\")\n",
    "\n",
    "# Train-Validation-Test Split\n",
    "# First, split into training and a temporary set for validation/test\n",
    "X_train_full, X_temp, y_train_full, y_temp = train_test_split(\n",
    "    X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
    ")\n",
    "# Then, split the temporary set into validation and test sets (50-50 split)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Tokenization and Padding\n",
    "max_words = 10000  \n",
    "max_len = 100  \n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<unk>\")\n",
    "tokenizer.fit_on_texts(X_train_full)\n",
    "\n",
    "# Convert text to sequences and pad to a fixed length\n",
    "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train_full), maxlen=max_len)\n",
    "X_val_seq = pad_sequences(tokenizer.texts_to_sequences(X_val), maxlen=max_len)\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0088eb78",
   "metadata": {},
   "source": [
    "Below the CNN-LSTM defined function has a maximum number of of words, a maximum length of the words  that appear together, by ngrams, ridge regression that shrinks coeeficients because we aim to retain them, and a dropuot rate.\n",
    "It has layers including, multi-channel CNN to capture the different sizes of the ngrams, an LSTM branch that recognizes patterns, the combined features for all the branches, and the feature vector layer. \n",
    "It compiles the modelusing the exponential learning rate and uses adanm as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f9af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_feature_extractor(max_words, max_len, embedding_dim=100, l2_reg=0.01, dropout_rate=0.5):\n",
    "    \n",
    "    input_layer = Input(shape=(max_len,), name='input_sequence')\n",
    "    embedding_layer = Embedding(max_words, embedding_dim, input_length=max_len, name='embedding')(input_layer)\n",
    "\n",
    "    # Multi-channel CNN for capturing different n-grams\n",
    "    conv_3_gram = Conv1D(64, 3, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_reg), name='conv_3_gram')(embedding_layer)\n",
    "    pool_3_gram = GlobalMaxPooling1D(name='pool_3_gram')(conv_3_gram)\n",
    "\n",
    "    conv_4_gram = Conv1D(64, 4, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_reg), name='conv_4_gram')(embedding_layer)\n",
    "    pool_4_gram = GlobalMaxPooling1D(name='pool_4_gram')(conv_4_gram)\n",
    "\n",
    "    conv_5_gram = Conv1D(64, 5, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_reg), name='conv_5_gram')(embedding_layer)\n",
    "    pool_5_gram = GlobalMaxPooling1D(name='pool_5_gram')(conv_5_gram)\n",
    "\n",
    "    # LSTM branch for sequential pattern recognition\n",
    "    lstm_layer = LSTM(64, return_sequences=False, kernel_regularizer=tf.keras.regularizers.l2(l2_reg), name='lstm')(embedding_layer)\n",
    "    lstm_layer = BatchNormalization(name='batch_norm_lstm')(lstm_layer)\n",
    "\n",
    "    # Concatenate features from all branches\n",
    "    merged = tf.keras.layers.concatenate([pool_3_gram, pool_4_gram, pool_5_gram, lstm_layer], name='concatenate_layers')\n",
    "    merged = Dropout(dropout_rate, name='dropout_merged')(merged)\n",
    "\n",
    "    # Final dense layer to generate feature vector for base models\n",
    "    feature_vector = Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_reg), name='feature_vector_output')(merged)\n",
    "    \n",
    "    # Keras model for feature extraction\n",
    "    model = Model(inputs=input_layer, outputs=feature_vector, name='CNN_LSTM_Feature_Extractor')\n",
    "    \n",
    "    # Compile the model\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=0.0005,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.8\n",
    "    )\n",
    "    optimizer = Adam(learning_rate=lr_schedule)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and summarize the deep learning model\n",
    "cnn_lstm_extractor = create_cnn_lstm_feature_extractor(\n",
    "    max_words=max_words,\n",
    "    max_len=max_len,\n",
    "    l2_reg=0.01, \n",
    "    dropout_rate=0.5 \n",
    ")\n",
    "\n",
    "cnn_lstm_extractor.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7248b794",
   "metadata": {},
   "source": [
    "In the next cell we define callbacks for ealry stopping, extract features from the trained CNN-LSTM, extract features from the previous models used for prediction above, initializing the models in this new context and retrain them on the new features generated by CNN-LSTM. \n",
    "The models are combined into a voting ensemble and the probabilities of output predictions of each of the base model taken into account, and the best model is produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e4f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for early stopping\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "history_cnn_lstm = cnn_lstm_extractor.fit(\n",
    "    X_train_seq, y_train_full,\n",
    "    validation_data=(X_val_seq, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Extracting features from the trained CNN-LSTM model\n",
    "X_train_features = cnn_lstm_extractor.predict(X_train_seq)\n",
    "X_val_features = cnn_lstm_extractor.predict(X_val_seq)\n",
    "X_test_features = cnn_lstm_extractor.predict(X_test_seq)\n",
    "\n",
    "# Extracting classifiers from the previous models\n",
    "lr_classifier = final_model.named_steps['clf']  # LogisticRegression\n",
    "rf_classifier = rdf.named_steps['model']  # RandomForest\n",
    "nb_classifier = multinomial_tuned.named_steps['model']  # MultinomialNB\n",
    "xgb_classifier = best_xgb\n",
    "\n",
    "# Create fresh instances \n",
    "import xgboost as xgb\n",
    "\n",
    "lr_classifier = LogisticRegression(C=10, max_iter=1000, solver='liblinear', random_state=42)\n",
    "rf_classifier = RandomForestClassifier(class_weight='balanced') \n",
    "nb_classifier = MultinomialNB(alpha=0.1)\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train the classifiers on CNN-LSTM features\n",
    "lr_classifier.fit(X_train_features, y_train_full)\n",
    "rf_classifier.fit(X_train_features, y_train_full)\n",
    "nb_classifier.fit(X_train_features, y_train_full)\n",
    "xgb_classifier.fit(X_train_features, y_train_full)\n",
    "\n",
    "# Combine the base models into a voting ensemble\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', lr_classifier),\n",
    "        ('nb1', rf_classifier),  \n",
    "        ('nb2', nb_classifier),\n",
    "        ('xgb', xgb_classifier)\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "ensemble_model.fit(X_train_features, y_train_full)\n",
    "\n",
    "print(\"Ensemble model trained successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51729e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on the training set\n",
    "ensemble_train_pred = ensemble_model.predict(X_train_features)\n",
    "\n",
    "# Predicting on the validation set\n",
    "ensemble_val_pred = ensemble_model.predict(X_val_features)\n",
    "\n",
    "# Predicting on the test set\n",
    "ensemble_test_pred = ensemble_model.predict(X_test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e59b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the ensemble model\n",
    "training_accuracy = accuracy_score(y_train_full, ensemble_train_pred)\n",
    "val_accuracy = accuracy_score(y_val, ensemble_val_pred)\n",
    "test_accuracy = accuracy_score(y_test, ensemble_test_pred)\n",
    "print('Ensemble model performance')\n",
    "print('Training Accuracy: {:.2f}%'.format(training_accuracy * 100))\n",
    "print('Validation Accuracy: {:.2f}%'.format(val_accuracy * 100))\n",
    "print('Test Accuracy: {:.2f}%'.format(test_accuracy * 100))\n",
    "print('\\nDifference between train and test accuracy: {:.4f}'.format(training_accuracy - test_accuracy))\n",
    "\n",
    "# Classification Report for Test Set\n",
    "print('\\nTest Set Classification Report:')\n",
    "print(classification_report(y_test, ensemble_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93ab167",
   "metadata": {},
   "source": [
    "# 5 Evaluation\n",
    "The purpose of this section is to assess the performance of all trained models both classical and deep learning on the unseen test dataset.\n",
    "By evaluating these models, we aim to:\n",
    "- Identify which approach most effectively captures tweet sentiment for Apple and Google.\n",
    "- Understand the trade-offs between model complexity, accuracy, and interpretability.\n",
    "- Provide data-driven justification for model selection in the final recommendations.\n",
    "All models are evaluated using consistent metrics to ensure fairness and comparability, we evaluate all trained models using both validation and test performance metrics to ensure that the final model generalizes well and is not overfitted to the training data.\n",
    "Each modelâ€™s predictive quality is assessed through:\n",
    "- Validation F1-score (weighted) â€“ the main success metric for selecting the optimal model.\n",
    "- Validation Precision (weighted) and Recall (weighted) â€“ to check how well the model balances positive and negative sentiment predictions.\n",
    "- Test scores for the chosen final model, confirming generalization on unseen data.\n",
    "This two-tier evaluation approach helps:\n",
    "- Prevent overfitting by choosing the model that performs best on validation (not just training).\n",
    "- Objectively compare models based on balanced metrics across all sentiment classes.\n",
    "\n",
    "# 5.1 Why Weighted Metrics?\n",
    "The sentiment datasets we used was imbalanced (e.g., more neutral tweets than positive or negative) \n",
    "# refer visualization 2 in this notebook.\n",
    "Using weighted metrics ensures that each class contributes proportionally to the final score\n",
    "\n",
    "# 5.1.2 Primary success metric: F1-score (weighted)\n",
    "Because it best balances the trade-off between precision and recall across sentiment categories, which is our other aim, from the success metric criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb52b9aa",
   "metadata": {},
   "source": [
    "# 5.2 Evaluating the models\n",
    "## 5.2.1 Logistic Regression\n",
    "### 5.2.1.1 Binary classification(logistic regression)\n",
    "The modelâ€™s performance is evaluated using weighted precision, recall, F1-score, and accuracy.\n",
    "These metrics provide a balanced view, particularly in the presence of slight class imbalances.\n",
    "The confusion matrix helps visualize where misclassifications occur between Positive and Negative sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1100ae58",
   "metadata": {},
   "source": [
    "### 5.2.1.2 Multiclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c403611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='viridis',xticklabels=mltlog.classes_, yticklabels=mltlog.classes_)\n",
    "plt.title('Confusion Matrix - Multiclassification logistic regression')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8714879",
   "metadata": {},
   "source": [
    "### 5.2.1.3 Randomized Logistic Regression\n",
    "This is used to try and counter the imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec0340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ddc4006",
   "metadata": {},
   "source": [
    "### 5.2.1.3 Randomized Logistic Regression with gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847313b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final_model.predict(X_test)\n",
    "# Evaluate\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compute weighted metrics for comparison\n",
    "r_precision_val = precision_score(y_test, y_pred, average='weighted')\n",
    "r_recall_val = recall_score(y_test, y_pred, average='weighted')\n",
    "r_f1_val = f1_score(y_test, y_pred, average='weighted')\n",
    "r_accuracy_val = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nWeighted Precision: {r_precision_val:.3f}\")\n",
    "print(f\"Weighted Recall: {r_recall_val:.3f}\")\n",
    "print(f\"Weighted F1-score: {r_f1_val:.3f}\")\n",
    "print(f\"Accuracy: {r_accuracy_val:.3f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='viridis',\n",
    "            xticklabels=final_model.classes_, yticklabels=final_model.classes_)\n",
    "plt.title('Confusion Matrix - Logistic Regression (Tuned)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee585ef",
   "metadata": {},
   "source": [
    "## 5.2.2 Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95a4e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_hat_test))\n",
    "\n",
    "# Compute weighted metrics for comparison\n",
    "precision_val = precision_score(y_test, y_hat_test, average='weighted')\n",
    "recall_val = recall_score(y_test, y_hat_test, average='weighted')\n",
    "f1_val = f1_score(y_test, y_hat_test, average='weighted')\n",
    "accuracy_val = accuracy_score(y_test, y_hat_test)\n",
    "\n",
    "print(f\"\\nWeighted Precision: {precision_val:.3f}\")\n",
    "print(f\"Weighted Recall: {recall_val:.3f}\")\n",
    "print(f\"Weighted F1-score: {f1_val:.3f}\")\n",
    "print(f\"Accuracy: {accuracy_val:.3f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_hat_test), annot=True, fmt='d', cmap='viridis',\n",
    "            xticklabels=rdf.classes_, yticklabels=rdf.classes_)\n",
    "plt.title('Confusion Matrix - Logistic Regression (Tuned)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb1c3de",
   "metadata": {},
   "source": [
    "## 5.2.3 Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa98d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_hat_test))\n",
    "\n",
    "# Compute weighted metrics for comparison\n",
    "precision_val = precision_score(y_test, y_hat_test, average='weighted')\n",
    "recall_val = recall_score(y_test, y_hat_test, average='weighted')\n",
    "f1_val = f1_score(y_test, y_hat_test, average='weighted')\n",
    "accuracy_val = accuracy_score(y_test, y_hat_test)\n",
    "\n",
    "print(f\"\\nWeighted Precision: {precision_val:.3f}\")\n",
    "print(f\"Weighted Recall: {recall_val:.3f}\")\n",
    "print(f\"Weighted F1-score: {f1_val:.3f}\")\n",
    "print(f\"Accuracy: {accuracy_val:.3f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_hat_test), annot=True, fmt='d', cmap='viridis',\n",
    "            xticklabels=nb.classes_, yticklabels=nb.classes_)\n",
    "plt.title('Confusion Matrix - Logistic Regression (Tuned)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42977874",
   "metadata": {},
   "source": [
    "# 5.2.4 XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c870e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating all metrics\n",
    "accuracy = accuracy_score(y_test, xgbtest_pred)\n",
    "\n",
    "# F1-Score (multiple averaging methods)\n",
    "f1_macro = f1_score(y_test, xgbtest_pred, average='macro')\n",
    "f1_micro = f1_score(y_test, xgbtest_pred, average='micro')\n",
    "f1_weighted = f1_score(y_test, xgbtest_pred, average='weighted')\n",
    "\n",
    "# Precision and Recall\n",
    "precision_macro = precision_score(y_test, xgbtest_pred, average='macro')\n",
    "precision_weighted = precision_score(y_test, xgbtest_pred, average='weighted')\n",
    "\n",
    "recall_macro = recall_score(y_test, xgbtest_pred, average='macro')\n",
    "recall_weighted = recall_score(y_test, xgbtest_pred, average='weighted')\n",
    "\n",
    "print(\"\\nAccuracy:\\n\", accuracy)\n",
    "print(\"\\nF1_Macro:\\n\",f1_macro)\n",
    "print(\"\\nF1_Micro:\\n\",f1_micro)\n",
    "print(\"\\nF1_Weighted:\\n\",f1_weighted)\n",
    "print(\"\\nPrecision Macro:\\n\",precision_macro)\n",
    "print(\"\\nPrecision Weighted:\\n\",precision_weighted)\n",
    "print(\"\\nRecall Macro:\\n\",recall_macro)\n",
    "print(\"\\nRecall weighted:\\n\",recall_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817884c2",
   "metadata": {},
   "source": [
    "# 5.2.5 CNN-LSTM Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
